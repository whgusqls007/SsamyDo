import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tqdm import tqdm
import pickle
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint


train_data = pd.read_excel("traindata.xls")
test_data = pd.read_excel("testdata.xlsx")

train_data["document"].nunique(), train_data["label"].nunique()
train_data.drop_duplicates(subset=["document"], inplace=True)
train_data = train_data.dropna(how="any")

train_data["document"] = train_data["document"].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
train_data["document"] = train_data["document"].str.replace("^ +", "")
train_data["document"].replace("", np.nan, inplace=True)
train_data = train_data.dropna(how="any")

test_data.drop_duplicates(subset=["document"], inplace=True)
test_data["document"] = test_data["document"].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
test_data["document"] = test_data["document"].str.replace("^ +", "")
test_data["document"].replace("", np.nan, inplace=True)
test_data = test_data.dropna(how="any")

stopwords = [
    "ì˜",
    "ê°€",
    "ì´",
    "ì€",
    "ë“¤",
    "ëŠ”",
    "ì¢€",
    "ì˜",
    "ê±",
    "ê³¼",
    "ë„",
    "ë¥¼",
    "ìœ¼ë¡œ",
    "ì",
    "ì—",
    "ì™€",
    "í•œ",
    "í•˜ë‹¤",
]

okt = Okt()

max_len = 25

X_train = []
for sentence in tqdm(train_data["document"]):
    tokenized_sentence = okt.morphs(sentence, stem=True)  # í† í°í™”
    stopwords_removed_sentence = [
        word for word in tokenized_sentence if not word in stopwords
    ]  # ë¶ˆìš©ì–´ ì œê±°
    X_train.append(stopwords_removed_sentence)

X_test = []
for sentence in tqdm(test_data["document"]):
    tokenized_sentence = okt.morphs(sentence, stem=True)  # í† í°í™”
    stopwords_removed_sentence = [
        word for word in tokenized_sentence if not word in stopwords
    ]  # ë¶ˆìš©ì–´ ì œê±°
    X_test.append(stopwords_removed_sentence)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

threshold = 3
total_cnt = len(tokenizer.word_index)  # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0  # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ lstmì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0  # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0  # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if value < threshold:
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print("ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :", total_cnt)
print("ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s" % (threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt) * 100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq) * 100)

# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ì„ ê³ ë ¤í•˜ì—¬ + 1
vocab_size = total_cnt - rare_cnt + 1
print("ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :", vocab_size)

tokenizer = Tokenizer(vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

y_train = np.array(train_data["label"])
y_test = np.array(test_data["label"])

drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]

# ë¹ˆ ìƒ˜í”Œë“¤ì„ ì œê±°
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)
print(len(X_train))
print(len(y_train))

print("ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :", max(len(review) for review in X_train))
print("ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :", sum(map(len, X_train)) / len(X_train))
plt.hist([len(review) for review in X_train], bins=50)
plt.xlabel("length of samples")
plt.ylabel("number of samples")
plt.show()


def below_threshold_len(max_len, nested_list):
    count = 0
    for sentence in nested_list:
        if len(sentence) <= max_len:
            count = count + 1
    print("ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s" % (max_len, (count / len(nested_list)) * 100))


below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# dfx = pd.DataFrame(X_train)
# dfx.to_csv("x_train.txt")

# dfy = pd.DataFrame(y_train)
# dfy.to_csv("y_train.txt")

# save tokenizer
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#####################

embedding_dim = 100
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(1, activation="sigmoid"))

es = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=4)
mc = ModelCheckpoint(
    "best_model.h5", monitor="val_acc", mode="max", verbose=1, save_best_only=True
)

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["acc"])
history = model.fit(
    X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2
)

model.save("./")

loaded_model = load_model("best_model.h5")
# print("\n í…ŒìŠ¤íŠ¸ ì •í™•ë„:", loaded_model.evaluate(X_test, y_test))


#Predict
y_prediction = model.predict(X_test)
y_prediction = np.argmax (y_prediction)
y_test=np.argmax(y_test)

#Create confusion matrix and normalizes it over predicted (columns)
result = confusion_matrix(y_test, y_prediction , normalize='pred')
print(result)

def sentiment_predict(new_sentence):
    with open('tokenizer.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)
    new_sentence = re.sub(r"[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "", new_sentence)
    new_sentence = okt.morphs(new_sentence, stem=True)  # í† í°í™”
    new_sentence = [word for word in new_sentence if not word in stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    encoded = tokenizer.texts_to_sequences([new_sentence])  # ì •ìˆ˜ ì¸ì½”ë”©
    pad_new = pad_sequences(encoded, maxlen=max_len)  # íŒ¨ë”©
    score = float(loaded_model.predict(pad_new))  # ì˜ˆì¸¡
    if score > 0.5:
        print("{:.2f}% í™•ë¥ ë¡œ ê³µì§€ì…ë‹ˆë‹¤.\n".format(score * 100))
    else:
        print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n".format((1 - score) * 100))


print(sentiment_predict("""@here 
ë¶€ìš¸ê²½ 1ë°˜ ì—¬ëŸ¬ë¶„~ ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”-!!
ğŸ”¸9ì‹œ ì „ ì…ì‹¤ì²´í¬/ì˜¤ì „ ê±´ê°•ì„¤ë¬¸ ì™„ë£Œí•´ì£¼ì‹œê³  ì•„ë˜ ê³µì§€ì‚¬í•­ ê¼­ í™•ì¸í•´ì£¼ì„¸ìš”.

##### **[ê³µì§€]**
1. ë‹¨ì²´ë³´í—˜ ê°€ì…
  - ê°€ì…ë°©ë²• : ì—ë“€ì‹¸í”¼ ê³µì§€ì‚¬í•­ å…§ ì²¨ë¶€íŒŒì¼ ì‘ì„±í•˜ì—¬ ì œì¶œ(ì•”í˜¸ ì„¤ì • : 0707)
  - ì œì¶œë°©ë²• : ì—ë“€ì‹¸í”¼ > ë§ˆì´ìº í¼ìŠ¤ > ì„œë¥˜ì œì¶œ > ê¸€ì“°ê¸°
  - ì œì¶œê¸°í•œ : 1/12(ìˆ˜)ê¹Œì§€
2. ìš°í¸ë¹„ ì§€ê¸‰
  - ì—ë“€ì‹¸í”¼ ê³µì§€ì‚¬í•­ >  "ìš°í¸ë¹„ ì§€ê¸‰" ê´€ë ¨ ê²Œì‹œ í™•ì¸ ì²¨ë¶€íŒŒì¼ ì‘ì„±
  - ì œì¶œë°©ë²• : ì—ë“€ì‹¸í”¼ > ë§ˆì´ìº í¼ìŠ¤ > ì„œë¥˜ì œì¶œ > íŒŒì¼ëª… : ì§€ì—­_ë°˜ _ì´ë¦„(ê¸ˆì•¡)   ex) ë¶€ìš¸ê²½_1ë°˜_ê¹€ì‹¸í”¼(3170)
  - ì œì¶œê¸°í•œ : 1/12(ìˆ˜)ê¹Œì§€
3. 7ê¸° ì…í•™ì‹ Webex ì„œí¬í„°ì¦ˆ ëª¨ì§‘
  - ì‹ ì²­ì¡°ê±´ : Webex ê°€ìƒë°°ê²½ ì ìš©ì´ ì›í™œí•œ êµìœ¡ìƒ
  - ëª¨ì§‘ì¸ì› : âœ¨ì„¸ ìë¦¬ ë‚¨ì•˜ìŠµë‹ˆë‹¤-!! / ì €ì—ê²Œ MMìœ¼ë¡œ ì‹ ì²­í•´ì£¼ì„¸ìš”!
  - ì‹ ì²­ê¸°í•œ : ~1/11(í™”) ì˜¤ëŠ˜ê¹Œì§€!!
4. ê° ë°˜ ë³„ ìì¹˜íšŒ ì„ ì¶œ
  - ëª¨ì§‘ì¸ì› : ë°˜ ë³„ ë°˜ì¥ 1ëª…, CA 1ëª…
  - ì‹ ì²­ê¸°ê°„ : ~1/14(ê¸ˆ) 15ì‹œê¹Œì§€ / ì €ì—ê²Œ MMìœ¼ë¡œ ì‹ ì²­í•´ì£¼ì„¸ìš”!"""))

print(sentiment_predict("""
                        @HERE
ì´ë²ˆ 3ì›”ë¶€í„° 'ì‚¼ì„± ì„ì§ì› ë©˜í† ë§ ê²Œì‹œíŒ'ì´ ì˜¤í”ˆëœê±° ì•Œê³  ìˆìœ¼ì‹œì£ ? :slightly_smiling_face:

 [ì—ë“€ì‹¸í”¼-ë©˜í† ë§ ê²Œì‹œíŒ] ì—ì„œëŠ” ë©˜í† ë‹˜ë“¤ì˜ ì§„ì‹¬ì–´ë¦° ë©˜í† ë§ì´ ì´ì–´ì§€ê³  ìˆëŠ”ë°ìš”~

ë©˜í† ë§ì„ ì§„í–‰í•˜ê³  ê³„ì‹  í˜„ì—… ì„ ë°°ì´ì, ì‚¼ì„± ì„ì§ì› ë©˜í† ë‹˜ë“¤ê³¼ ì§ì ‘ ë§Œë‚  ìˆ˜ ìˆëŠ” ìë¦¬ë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.

ê°„ë‹´íšŒë¥¼ í†µí•´ í‰ì†Œ ë©˜í† ë‹˜ë“¤ê»˜ ê¶ê¸ˆí–ˆë˜ ë¶€ë¶„, ì·¨ì—…/ì§„ë¡œì— ê´€í•œ ë¶€ë¶„, ì—…ê³„ì— ê´€í•œ ë¶€ë¶„ ë“± ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤!

SWê°œë°œìë¡œ í•œì¸µ ë” ì„±ì¥í•  ìˆ˜ ìˆëŠ” ì•„ì£¼ì•„ì£¼ íŠ¹ë³„í•œ ì†Œí†µì˜ ì‹œê°„, ë†“ì¹˜ì§€ ë§ê³  ì ê·¹ ì°¸ì—¬ ë¶€íƒë“œë ¤ìš”~!

 
**â˜… ê°„ë‹´íšŒ ì°¸ì—¬ì‹ ì²­ : êµ¬ê¸€ ì„¤ë¬¸í¼ https://forms.gle/RAeE4N1UbY18oRbn8**

â˜…â˜… ì°¸ì„ì„ í¬ë§í•˜ì‹œëŠ” ë¶„ë“¤ì€ ~3/23(ìˆ˜)ê¹Œì§€ ì‹ ì²­í•´ì£¼ì„¸ìš” :slightly_smiling_face:

 

**â–¡ ê°„ë‹´íšŒ ì¼ì • : 3/29(í™”) or 3/30(ìˆ˜) ì˜¤í›„ 6ì‹œ~ (1H)**
â€» ë‹¨, 3/30(ìˆ˜)ëŠ” ì„ ì°©ìˆœìœ¼ë¡œ ë©˜í‹° ì°¸ì„ ì¸ì›ì´ í•™ê¸°ë³„ 8ëª…ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤.

â–¡ ì§„í–‰ë°©ì‹ : Webex(ì˜¨ë¼ì¸)"""))

print(sentiment_predict(
"""@here 
1ë°˜ ì—¬ëŸ¬ë¶„~ ì €ë²ˆì£¼ ì¢…ë¡€ ë•Œ ì „í•´ë“œë¦° ê°œì¸ì •ë³´ ë™ì˜ ì•ˆí•˜ì‹  ë¶„ë“¤ì´ ë§ë„¤ìš”ã…œã…œ
ì•„ë˜ ê²½ë¡œë¡œ 13ì‹œê¹Œì§€ ê¼­! í•˜ì…”ì•¼ í•©ë‹ˆë‹¤ :blush: 
í•™ì‚¬ì‚¬ì´íŠ¸ > ë§ˆì´ìº í¼ìŠ¤ > êµìœ¡ìƒ ì„œì•½ì„œ > 'ì œ3ì ì •ë³´ì œê³µ ë™ì˜ì„œ(ì¹´ì¹´ì˜¤)' í´ë¦­ > íœ´ëŒ€í° ì¸ì¦ë§Œ í•˜ë©´ ë!"""))


print(sentiment_predict(
"""@here 
1ë°˜ ì—¬ëŸ¬ë¶„~ ì € ë°°ê³ íŒŒìš” í”„ë¡ íŠ¸ê°€ ë‹¤ í•´ ì¤˜"""))

print(sentiment_predict("@here ì•ˆë…•í•˜ì„¸ìš” 1ë°˜ ì—¬ëŸ¬ë¶„~ ì €ë²ˆì£¼ ì¢…ë¡€ ë•Œ ì „í•´ë“œë¦° ê°œì¸ì •ë³´ ë™ì˜ 1/24ì¼ê¹Œì§€ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ì œê°€ ë¶€íƒ ì¢€ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."))



    
exit()